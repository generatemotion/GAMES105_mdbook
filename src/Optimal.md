# Lecture 12

P2   
## Outline   

 - Optimal Control   
 - Model-based Approaches vs. Model-free Approaches   
 - Sampling-based Optimization   
 - Reinforcement Learning   

 - Conclusion  


P3   
## Recap: Trajectory Optimization

![](./assets/12-01.png)


P4  
## Recap: Feedforward Control

![](./assets/12-02.png)   


P5   
## Recap: Feedback Control


![](./assets/12-03.png)  


P7   
## Recap: Feedback Control

![](./assets/12-04.png)  


> &#x2705; å‰é¦ˆæ§åˆ¶ä¼˜åŒ–çš„æ˜¯è½¨è¿¹ã€‚  
> &#x2705; åé¦ˆæ§åˆ¶ä¼˜åŒ–çš„æ˜¯æ§åˆ¶ç­–ç•¥ï¼Œæ§åˆ¶ç­–ç•¥æ˜¯ä¸€ä¸ªå‡½æ•°ï¼Œæ ¹æ®å½“å‰çŠ¶æ€ä¼˜åŒ–è½¨è¿¹ã€‚  



P9  

![](./assets/12-05.png)  


> &#x2705; Feedbackç±»ä¼¼æ„é€ ä¸€ä¸ªåœºï¼ŒæŠŠä»»ä½•çŠ¶æ€æ¨åˆ°ç›®æ ‡çŠ¶æ€ã€‚   



P10  
## Constrained Optimization

$$
\begin{matrix}
 \min_{x}  f(x)\\\\
ğ‘ .ğ‘¡. g(x)=0
\end{matrix}
$$

![](./assets/12-06.png)    


P12  
## Constrained Optimization  

Soft constraint?   

$$
\min_{x}  f(x)+ wg(x)
$$

\\(^\ast \\) The solution \\(x^\ast\\)  may not satisfy the constraint   

 
P16   
## Lagrange Multiplier    

![](./assets/12-08.png)  

![](./assets/12-07.png)  

Lagrange function   

$$
L(x,\lambda )=f(x)+\lambda ^Tg(x)
$$



> &#x2705; å……åˆ†éå¿…è¦æ¡ä»¶ã€‚  
> &#x2705; æ‹‰æ ¼æœ—æ—¥ä¹˜å­æ³•ã€‚  




P18   
## Lagrange Multiplier 


![](./assets/12-09-1.png)  


P20   
## Solving Trajectory Optimization Problem   

Find a control sequence {\\(a_t\\)} that generates a state sequence {\\(s_t\\)} start from \\(s_o\\) minimizes    

$$
\min h (s_r)+\sum _{t=0}^{T-1} h(s_t,a_t)
$$


subject to   

$$
\begin{matrix}
 f(s_t,a_t)-s_{t+1}=0\\\\
\text{ for } 0 \le t < T
\end{matrix}
$$

The Lagrange function   

$$
L(s,a,\lambda ) = h(s _ T)+ \sum _ {t=0} ^ {T-1} h(s _t,a _t) + \lambda _ {t+1}^T(f(s _t,a _t) - s _ {t+1})
$$


P21  

> &#x2705; å› ä¸ºæŠŠæ—¶é—´ç¦»æ•£åŒ–ï¼Œæ­¤å¤„ç”¨æ±‚å’Œä¸ç”¨ç§¯åˆ†ã€‚    



P27   
## Solving Trajectory Optimization Problem  


![](./assets/12-10-1.png)  


> &#x2705; æ‹‰æ ¼æœ—æ—¥æ–¹ç¨‹ï¼Œå¯¹æ¯ä¸ªå˜é‡æ±‚å¯¼ï¼Œå¹¶ä»¤å¯¼æ•°ä¸ºé›¶ã€‚  
> &#x2705; å› æ­¤å¾—åˆ°æ–¹ç¨‹ç»„ã€‚



P30  
## Pontryaginâ€™s Maximum Principle for discrete systems


![](./assets/12-11.png)  

![](./assets/12-12.png)  


> &#x2705; æ–¹ç¨‹ç»„æ•´ç†å¾—åˆ°å·¦è¾¹ï¼Œç§°ä¸ºPMPæ¡ä»¶ã€‚  



P32   
## Optimal Control

**Open-loop Control**:    
given a start state \\(s_0\\), compute sequence of actions {\\(a_t\\)} to reach the goal   


![](./assets/12-13.png)  

---  

>  **Shooting method** directly applies PMP. However, it does not scale well to complicated problems such as motion controlâ€¦   
\\(<br>\\)   
Need to be combined with collocation method, multiple shooting, etc. for those problems.    
\\(<br>\\)   
Or use derivative-free approaches.   

![](./assets/12-14.png)  


P34  
## Dynamic Programming   

![](./assets/12-15.png) 

Find a path {\\(s_t\\)} that minimizes    


$$
J(s_0)=\sum _ {t=0}^{ } h(s_t,s_{t+1})
$$


> &#x2705; åŠ¨æ€è§„åˆ’é—®é¢˜ã€‚  



P35  
## Dynamic Programming   

Find a sequence of action {\\(a_t\\)} that minimizes   

$$
J(s_0)=\sum _ {t=0}^{ } h(s_t,a_t)
$$

subject to   

$$
s_{t+1}=f(s_t,a_t)
$$


> &#x2705; è½¨è¿¹é—®é¢˜ã€‚  



P36  
## Dynamic Programming   

Find a policy \\( a_t=\pi (s_t,t)\\) that minimizes    

$$
J(s_0)=\sum _ {t=0}^{ } h(s_t,a_t)
$$


subject to   

$$
s_{t+1}=f(s_t,a_t)
$$



> &#x2705; æ§åˆ¶ç­–ç•¥é—®é¢˜ã€‚  


P37   
## Dynamic Programming   

Find a policy \\( a_t=\pi (s_t)\\) that minimizes    

$$
J(s_0)=\sum _ {t=0}^{ } h(s_t,a_t)
$$


subject to   

$$
s_{t+1}=f(s_t,a_t)
$$


P39   
## Bellmanâ€™s Principle of Optimality   

![](./assets/12-16.png) 

An optimal policy has the property that whatever the initial 
state and initial decision are, the remaining decisions must 
constitute an optimal policy with regard to the state resulting 
from the first decision.   

\\(^\ast \\) The problem is said to have optimal substructure    


P40   
## Bellmanâ€™s Principle of Optimality  

Value of a state \\(V(s)\\) :    

 - the minimal total cost for finishing the task starting from \\(s\\)   
\\(\Updownarrow \\)
 - the total cost for finishing the task starting from \\(s\\) using the optimal policy    



> &#x2705; Value Funcronï¼Œè®¡ç®—ä»æŸä¸ªç»“ç‚¹åˆ° gool çš„æœ€å°ä»£ä»·ã€‚   
> &#x2705; åé¢åŠ¨æ€è§„åˆ’åŸç†è·³è¿‡ã€‚   



P49   
## The Bellman Equation   

Mathematically, an optimal **value function** \\(V(s)\\) can be defined recursively as:   

$$
V(s)=\min_{a} (h(s,a)+V(f(s,a)))
$$

If we know this value function, the optimal **policy** can be computed as   

$$
\pi (s)=\arg \min_{a} (h(s,a)+V(f(s,a)))
$$

or   

$$
\begin{matrix}
 \pi (s)=\arg \min_{a} Q(s,a)\\\\
\text{where} \quad \quad  Q(s,a)=h(s,a)+V(f(s,a))
\end{matrix}
$$


Q-function State-action value function    


P50   
## The Bellman Equation   

Mathematically, an optimal value function \\(V(s)\\) can be defined recursively as:   

$$
V(s)=\min_{a} (h(s,a)+V(f(s,a)))
$$

Learning \\(V(s)\\) and/or \\(Q(s,a)\\) is the core of optimal control / reinforcement learning methods   

\\(<br>\\)   

If we know this value function, the optimal policy can be computed as

$$
\pi (s)=\arg \min_{a} (h(s,a)+V(f(s,a)))
$$

This arg max can be easily computed for discrete control problems.   
But there are not always closed-forms solution for continuous control problems.   

\\(<br>\\)   

or   

$$
\begin{matrix}
 \pi (s)=\arg \min_{a} Q(s,a)\\\\
\text{where} \quad \quad  Q(s,a)=h(s,a)+V(f(s,a))
\end{matrix}
$$

Q-function State-action value function    


> &#x2705; å¼ºåŒ–å­¦ä¹ æœ€ä¸»è¦çš„ç›®çš„æ˜¯å­¦ä¹  \\(V\\) å‡½æ•°å’Œ \\(Q\\) å‡½æ•°ï¼Œå¦‚æœ \\(a\\) æ˜¯æœ‰é™çŠ¶æ€ï¼Œéå†å³å¯ã€‚  
> &#x2705; ä½†åœ¨è§’è‰²åŠ¨ç”»é‡Œï¼Œ\\(a\\) æ˜¯è¿ç»­çŠ¶æ€ã€‚  



P52   
## Linear Quadratic Regulator (LQR)   

![](./assets/12-17.png) 


 - LQR is a special class of optimal control problems with   
    - **Linear** dynamic function   
    - **Quadratic** objective function   


> &#x2705; LQR æ˜¯æ§åˆ¶é¢†åŸŸä¸€ç±»ç»å…¸é—®é¢˜ï¼Œå®ƒå¯¹åŸæ§åˆ¶é—®é¢˜åšäº†ä¸€äº›ç‰¹å®šçš„çº¦æŸã€‚



P53   
## A very simple example   

![](./assets/12-18.png) 

Compute a target trajectory \\(\tilde{x}(t)\\) such that the simulated trajectory \\(x(t)\\) is a sine curve.   


![](./assets/12-19.png) 

$$
\min _{(x_n,v_n,\tilde{x} _n)} \sum _{n=0}^{N} (\sin (t_n)-x_n)^2+\sum _{n=0}^{N}\tilde{x}^2_n 
$$

$$
\begin{align*}
 s.t. \quad \quad v _ {n+1} & = v _ n + h(k _p ( \tilde{x} _ n - x _ n) - k _ dv _ n ) \\\\
 v _ {x+1} & = x _ n + hv _ {n+1}
\end{align*}
$$


P54  
objective function   

$$
\min s^T_TQ_Ts_T+\sum_{t=0}^{T} s^T_tQ_ts_t+a^T_tR_ta_t
$$

subject to   

$$
s_{t+1}=A_ts_t+B_ta_t   \quad \quad \text{for }   0\le t <T 
$$

dynamic function   

P58   
## Linear Quadratic Regulator (LQR)

![](./assets/12-20-1.png) 


P60   
## Linear Quadratic Regulator (LQR)

![](./assets/12-20.png) 


P61 
## Linear Quadratic Regulator (LQR)


![](./assets/12-21.png) 

P62   
## Linear Quadratic Regulator (LQR)

![](./assets/12-22.png)  


P63  
## Linear Quadratic Regulator (LQR)


![](./assets/12-23.png) 

P64   
## Linear Quadratic Regulator (LQR)

 - LQR is a special class of optimal control problems with   
    - Linear dynamic function   
    - Quadratic objective function   
 - Solution of LQR is a linear feedback policy  

![](./assets/12-24.png) 


P65   
## Linear Quadratic Regulator (LQR)

 - How to deal with   
    - Nonlinear dynamic function?   
    - Non-quadratic objective function?   


> &#x2705; äººä½“è¿åŠ¨æ¶‰åŠåˆ°è§’åº¦æ—‹è½¬ï¼Œå› æ­¤æ˜¯éçº¿æ€§çš„ã€‚  



P68  
## Linear Quadratic Regulator (LQR)  

 - Nonlinear problems   

![](./assets/12-25.png) 

Approximate cost function as a quadratic function:   

$$
h(s_t,a_t)\approx h(\bar{s}_t ,\bar{a}_t)+\nabla h(\bar{s}_t ,\bar{a}_t)\begin{bmatrix}
 s_t-\bar{s} _t\\\\
a_t-\bar{a} _t
\end{bmatrix} + \frac{1}{2} \begin{bmatrix}
 s_t-\bar{s} _t\\\\
a_t-\bar{a} _t
\end{bmatrix}^T\nabla^2h(\bar{s}_t ,\bar{a}_t)\begin{bmatrix}
 s_t-\bar{s} _t\\\\
a_t-\bar{a} _t
\end{bmatrix}
$$

Approximate dynamic function as a linear function:    

$$
f(s_t,a_t)\approx f(\bar{s}_t ,\bar{a}_t)+\nabla f(\bar{s}_t ,\bar{a}_t)\begin{bmatrix}
 s_t-\bar{s} _t\\\\
a_t-\bar{a} _t
\end{bmatrix}
$$

iLQR: iterative LQR 


Or a quadratic function:   

$$
f(s_t,a_t)\approx \ast \ast \ast \frac{1}{2} \begin{bmatrix}
 s_t-\bar{s} _t\\\\
a_t-\bar{a} _t
\end{bmatrix}^T\nabla^2f(\bar{s}_t ,\bar{a}_t)\begin{bmatrix}
 s_t-\bar{s} _t\\\\
a_t-\bar{a} _t
\end{bmatrix}
$$

DDP: Differential Dynamic Programming  


> &#x2705; æ–¹æ³•ï¼šæŠŠé—®é¢˜è¿‘ä¼¼ä¸ºçº¿æ€§é—®é¢˜ã€‚  
> &#x2705; ç›®æ ‡å‡½æ•°ï¼šæ³°å‹’å±•å¼€ï¼Œä¿ç•™äºŒæ¬¡ã€‚  
> &#x2705; è½¬ç§»å‡½æ•°ï¼šæ³°å‹’å±•å¼€ï¼Œä¿ç•™ä¸€æ¬¡æˆ–äºŒæ¬¡ã€‚  



P69  
## Locomotion Using Optimal Control

[Muico et al 2011 - Composite Control of Physically Simulated Characters]   


> &#x2705; é€‰æ‹©åˆé€‚çš„ \\(Q\\) å’Œ \\(R\\)ï¼Œéœ€è¦ä¸€äº›å·¥ç¨‹ä¸Šçš„æŠ€å·§ã€‚   
> &#x2705; ä¸ºäº†æ±‚è§£æ–¹ç¨‹ï¼Œéœ€è¦æ˜¾å¼åœ°å»ºæ¨¡è¿åŠ¨å­¦æ–¹ç¨‹ã€‚  



P70  
## Model-based Method vs. Model-free Method   

> What if the dynamic function \\(f(s,a)\\) is not know?  

> What if the dynamic function \\(f(s,a)\\) is not accurate?    

> What if the system has noise?    

> What if the system is highly nonlinear?     



> &#x2705; \\(f\\) æœªçŸ¥åªæ˜¯æŠŠ \\(f\\) å½“æˆä¸€ä¸ªé»‘ç›’å­ï¼Œä»éœ€è¦æ ¹æ® \\(S_t\\) å¾—åˆ° \\(S_{tï¼‹1}\\) .   



P72  
## Sampling-based Policy Optimization   

 - Iterative methods
    - Goal: find the optimal **policy** \\(\pi (s;\theta )\\) that minimize the objective \\(J(\theta )=\sum_{t=0}^{}h(s_t,a_t) \\)     
    - Initialize policy parmeters \\(\pi (x;\theta )\\)   
    - Repeat:   
      - Propose a set of candidate parameters {\\(\theta _i \\)} according to \\(\theta \\)    
      - Simulate the agent under the control of each \\( \pi ( \theta _i)\\) 
      - Evaluate the objective function \\( J (\theta_i )\\)  on the simulated state-action sequences    
      - Update the estimation of \\(\theta \\) based on {\\( J (\theta_i )\\)}     

 - Example: CMA-ES


> &#x2705; åŸºäºé‡‡æ ·çš„æ–¹æ³•ã€‚  



P73   
## Example: Locomotion Controller with Linear Policy

[Liu et al. 2012 â€“ Terrain Runner]


P74  
## Stage 1a: Open-loop Policy   

Find open-loop control using SAMCON

![](./assets/12-26.png) 


> &#x2705; ä½¿ç”¨å¼€ç¯è½¨è¿¹ä¼˜åŒ–å¾—åˆ°å¼€ç¯æ§åˆ¶è½¨è¿¹ã€‚    



P76  
## Stage 1b: Linear Feedback Policy

![](./assets/12-27.png)   


> &#x2705; ä½¿ç”¨åé¦ˆæ§åˆ¶æ›´æ–°æ§åˆ¶ä¿¡å·ã€‚ç”±äºå‡è®¾äº†çº¿æ€§å…³ç³»ï¼Œæ ¹æ®åç¦»offsetå¯ç›´æ¥å¾—åˆ°è°ƒæ•´offset.  




P78   
## Stage 1b: Reduced-order Closed-loop Policy

![](./assets/12-28.png)  



> &#x2705; æŠŠ \\(M\\) åˆ†è§£ä¸ºä¸¤ä¸ªçŸ©é˜µï¼Œ\\(M_{AXB}=M_{AXC}\cdot M_{CXB}\\) å¦‚æœ \\(C\\) æ¯”è¾ƒå°ï¼Œå¯ä»¥æ˜æ˜¾å‡å°‘çŸ©é˜µçš„å‚æ•°é‡ã€‚    
> &#x2705; å¥½å¤„ï¼š(1) å‡å°‘å‚æ•°ï¼Œå‡åŒ–ä¼˜åŒ–è¿‡ç¨‹ã€‚  
> &#x2705; (2) æŠ¹æ‰çŠ¶æ€é‡Œä¸éœ€è¦çš„ä¿¡æ¯ã€‚  



P79   
## Manually-selected States: s   

 - Running: 12 dimensions


![](./assets/12-29.png) 


> &#x2705; ï¼ˆ1ï¼‰æ ¹ç»“ç‚¹æ—‹è½¬ï¼ˆ2ï¼‰è´¨å¿ƒä½ç½®ï¼ˆ3ï¼‰è´¨å¿ƒé€Ÿåº¦ï¼ˆ4ï¼‰æ”¯æ’‘è„šä½ç½®   



P80  
## Manually-selected Controls: a  

 - for all skills: 9 dimensions   


![](./assets/12-30.png)   


> &#x2705; ä»…å¯¹å°‘æ•°å…³èŠ‚åŠ åé¦ˆã€‚   



P81   
## Optimization

$$
\delta a=M\delta s+\hat{a} 
$$

 - Optimize \\(M\\)   
    - CMA, Covariance Matrix Adaption ([Hansen 2006])
    - For the running task:
      - #optimization variables: \\(12 ^\ast 9 = 108 / (12^\ast 3+3 ^\ast 9) = 63\\)   
     - 12 minutes on 24 cores   


P82   
## Example: Locomotion Controller with Linear Policy

[Liu et al. 2012 â€“ Terrain Runner]   


P85  
## Optimal Control \\(\Leftrightarrow \\) Reinforcement Learning   


â€¢ RL shares roughly the same overall goal with Optimal Control   

$$
\max \sum_{t=0}^{} r (s_t,a_t)
$$

â€¢ But RL typically does not assume perfect knowledge of system   

 ![](./assets/12-30-1.png) 

 - RL can still take advantage of a system model â†’ model-based RL   
    - The model can be learned from data   
 $$
 s_{t+1}=f(s_t,a_t;\theta )
 $$



> &#x2705; æœ€ä¼˜æ§åˆ¶è¦æ±‚æœ‰ç²¾ç¡®çš„è¿åŠ¨æ–¹ç¨‹ï¼Œè€ŒRLä¸éœ€è¦ã€‚   
> &#x2705; RLé€šè¿‡ä¸æ–­ä¸ä¸–ç•Œäº¤äº’è¿›è¡Œé‡‡æ ·ã€‚   


P87   
## Markov Decision Process (MDP)  


![](./assets/12-32.png)   


![](./assets/12-31.png)   

State  \\(\quad s_t \quad \quad \\)Action \\(\quad a_t\\)

Policy \\(\quad \quad a_t\sim \pi (\cdot \mid s_t)\\)   

Transition probability  \\(\quad \quad s_{t+1}\sim p  (\cdot \mid s_t,a_t)\\)   

Reward  \\(\quad \quad r_t=r (s_t,a_t)\\)   

$$
\text{Return }\quad \quad R = \sum _{t}^{} \gamma ^t r (s_t,a_t)\quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad 
$$   



> &#x2705; çœŸå®åœºæ™¯ä¸­è½¨è¿¹æ— é™é•¿ï¼Œä¼šå¯¼åˆ° \\(r\\) æ— é™å¤§ã€‚   
> &#x2705; å› æ­¤ä¼šä½¿ç”¨å°äº 1 çš„ \\(r,t\\) è¶Šå¤§åˆ™å¯¹ç»“æœçš„å½±å“è¶Šå°ã€‚   



P88  
## Markov Decision Process (MDP)   

Trajectory    

$$
\begin{matrix}
  \tau =& s_0 & a_0 & s_1 & a_1 & s_2&\dots 
\end{matrix}
$$

Reward   

![](./assets/12-33.png)   



P90   
## Markov Decision Process (MDP)

MDP is a **discrete-time** stochastic control process.    
It provides a mathematical framework for modeling decision making in situations     
where outcomes are **partly random and partly under the control of a decision maker**.   

A MDP problem:    

\\(\mathcal{M}\\) = {\\(S,A,p,r\\)}    
\\(S\\): state space   
\\(A\\): action space   


> &#x2705; Markovæ€§è´¨ï¼šå½“å½“å‰çŠ¶æ€å·²çŸ¥çš„æƒ…å†µä¸‹ï¼Œä¸‹ä¸€æ—¶åˆ»çŠ¶æ€åªä¸å½“å‰çŠ¶æ€ç›¸å…³ï¼Œè€Œä¸ä¸ä¹‹å‰ä»»ä¸€æ—¶åˆ»çŠ¶æ€ç›¸å…³ã€‚   
> &#x2705; pï¼šçŠ¶æ€è½¬ç§»æ¦‚ç‡ï¼Œå³è¿åŠ¨å­¦æ–¹ç¨‹ã€‚   
> &#x2705; rï¼šä»£ä»·å‡½æ•°ã€‚   



P91  
## Markov Decision Process (MDP)   

A MDP problem:    

\\(\mathcal{M}\\) = {\\(S,A,p,r\\)}   

Solve for a policy \\(\pi (a\mid s)\\) that optimize the **expected return**    


$$
J=E[R]=E_{\tau \sim \pi }[\sum_{t}^{} \gamma ^tr(s_t,a_t)]
$$

Overall all trajectories \\(\tau \\) = { \\(s_0, a_0 , s_1 , a_1 ,  \dots  \\)} induced by \\(\pi \\)   



> &#x2705; ä½¿æœŸæœ›æœ€ä¼˜ï¼Œè€Œä¸æ˜¯ç›´æ¥æ‰¾æœ€ä¼˜è§£ã€‚   
> &#x2705; å‡è®¾ \\(\pi \\) å‡½æ•°å’Œ \\(p\\) å‡½æ•°éƒ½æ˜¯æœ‰å™ªéŸ³çš„ï¼Œå³å¾—åˆ°çš„ç»“æœä¸æ˜¯ç¡®å®šå€¼ï¼Œè€Œæ˜¯ä»¥ä¸€å®šæ¦‚ç‡å¾—åˆ°æŸä¸ªç»“æœã€‚   



P93   
## Bellman Equations

In optimal control:    

![](./assets/12-34.png)   

In RL control:    

![](./assets/12-35.png)   


P94   
## How to Solve MDP  

 - Value-based Methods
    - Learning the value function/Q-function using the Bellman equations   
    - Evaluation the policy as    

    $$
    \pi (s) = \arg \min_{a} Q(s,a)
    $$

    - Typically used for discrete problems   
    - Example: Value iteration, Q-l a ning, DQN, â€¦   



P95   
## How to Solve MDP  

DQN [Mnih et al. 2015, Human-level control through deep reinforcement learning]   

P96  
## Multi-skill Characters


![](./assets/12-36.png)   

[Liu et al. 2017: Learning to Schedule Control Fragments ]   


> &#x2705; DQN æ–¹æ³•è¦æ±‚æ§åˆ¶ç©ºé—´å¿…é¡»æ˜¯ç¦»æ•£çš„ï¼Œä½†çŠ¶æ€ç©ºé—´å¯ä»¥æ˜¯è¿ç»­çš„ã€‚  
> &#x2705; å› æ­¤å¯ç”¨äºé«˜é˜¶çš„æ§åˆ¶ã€‚  



P97   
## How to Solve MDP   

 - Policy Gradient approach
    - Learning the value function/Q-function using the Bellman equations   
    - Compute approximate **policy gradient** according to value functions using Monte-Carlo method   

    - Update the policy using policy gradient  

    - Suitable for **continuous** problems   

    - Exa pl : REINFORCE, TRPO, PPO, â€¦   



> &#x2705; policy grodient æ˜¯ Value function å¯¹çŠ¶æ€å‚æ•°çš„æ±‚å¯¼ã€‚ä½†è¿™ä¸ªæ²¡æ³•ç®—ï¼Œæ‰€ä»¥ç”¨ç»Ÿè®¡çš„æ–¹æ³•å¾—åˆ°è¿‘ä¼¼ã€‚



P98   
## How to Solve MDP

||||
|--|--|--|
| ![](./assets/12-37.png)   |  ![](./assets/12-38.png)   |  ![](./assets/12-39.png)   | 
| [Liu et al. 2016. ControlGraphs] | [Liu et al. 2018]   | [Peng et al. 2018. DeepMimic] |  



P100  
## Generative Control Policies   

[Yao et al. Control VAE]   



P101  
## Whatâ€™s Next?   

 - Digital Cerebellum â€“ Large Pretrained Model for Motion Control

![](./assets/12-40.png)   


P102  
## Whatâ€™s Next?    

 - Cross-modality Generation
    - \\(\Leftrightarrow\\) LLM \\(\Leftrightarrow\\) Text/Audio \\(\Leftrightarrow\\) Motion/Control \\(\Leftrightarrow\\) Image/Video \\(\Leftrightarrow\\)     
    - Digital Actor?    


---------------------------------------
> æœ¬æ–‡å‡ºè‡ªCaterpillarStudyGroupï¼Œè½¬è½½è¯·æ³¨æ˜å‡ºå¤„ã€‚
>
> https://caterpillarstudygroup.github.io/GAMES105_mdbook/