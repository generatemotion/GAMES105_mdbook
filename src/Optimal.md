P2   
# Outline   

 - Optimal Control   
 - Model-based Approaches vs. Model-free Approaches   
 - Sampling-based Optimization   
 - Reinforcement Learning   

 - Conclusion  


P3   
## Recap

|feedforward|feedback|
|---|---|
|![](./assets/12-01.png)|![](./assets/12-04.png)  |
|![](./assets/12-02.png)|![](./assets/12-03.png) |

> &#x2705; å¼€ç¯æ§åˆ¶ï¼šåªè€ƒè™‘åˆå§‹çŠ¶æ€ã€‚   
> &#x2705; å‰é¦ˆæ§åˆ¶ï¼šè€ƒè™‘åˆå§‹çŠ¶æ€å’Œå¹²æŒ ã€‚   
> &#x2705; å‰é¦ˆæ§åˆ¶ä¼˜åŒ–çš„æ˜¯è½¨è¿¹ã€‚  
> &#x2705; åé¦ˆæ§åˆ¶ä¼˜åŒ–çš„æ˜¯æ§åˆ¶ç­–ç•¥ï¼Œæ§åˆ¶ç­–ç•¥æ˜¯ä¸€ä¸ªå‡½æ•°ï¼Œæ ¹æ®å½“å‰çŠ¶æ€ä¼˜åŒ–è½¨è¿¹ã€‚  

P9  

![](./assets/12-05.png)  

> &#x2705; Feedback ç±»ä¼¼æ„é€ ä¸€ä¸ªåœºï¼ŒæŠŠä»»ä½•çŠ¶æ€æ¨åˆ°ç›®æ ‡çŠ¶æ€ã€‚   



P10  
# å¼€ç¯æ§åˆ¶

## é—®é¢˜æè¿°

$$
\begin{matrix}
 \min_{x}  f(x)\\\\
ğ‘ .ğ‘¡. g(x)=0
\end{matrix}
$$

![](./assets/12-06.png)    


P12  
## æŠŠç¡¬çº¦æŸè½¬åŒ–ä¸ºè½¯çº¦æŸ  

$$
\min_{x}  f(x)+ wg(x)
$$

\\(^\ast \\) The solution \\(x^\ast\\)  may not satisfy the constraint   

 
P16   
## Lagrange Multiplier - æŠŠçº¦æŸæ¡ä»¶è½¬åŒ–ä¸ºä¼˜åŒ–    

> &#x2705; æ‹‰æ ¼æœ—æ—¥ä¹˜å­æ³•ã€‚  

![](./assets/12-08.png)  

> &#x2705; é€šè¿‡è§‚å¯Ÿå¯çŸ¥ï¼Œæå€¼ç‚¹ä½äº\\({f}'(x)\\) ä¸ \\(g\\) çš„åˆ‡çº¿å‚ç›´ï¼Œå³ \\({f}' (x)\\) ä¸ \\({g}' (x)\\) å¹³è¡Œã€‚ï¼ˆå……åˆ†éå¿…è¦æ¡ä»¶ã€‚ï¼‰   

å› æ­¤ï¼š  

![](./assets/12-07.png)  

Lagrange function   

$$
L(x,\lambda )=f(x)+\lambda ^Tg(x)
$$

> &#x2705; æŠŠçº¦æŸæ¡ä»¶è½¬åŒ–ä¸ºä¼˜åŒ–ã€‚   

P18   
## Lagrange Multiplier 

![](./assets/12-09-1.png)  

> &#x2705; è¿™æ˜¯ä¸€ä¸ªä¼˜åŒ–é—®é¢˜ï¼Œé€šè¿‡æ¢¯åº¦ä¸‹é™æ‰¾åˆ°æå€¼ç‚¹ã€‚  



P20   
## Solving Trajectory Optimization Problem   

### å®šä¹‰å¸¦çº¦æŸçš„ä¼˜åŒ–é—®é¢˜

Find a control sequence {\\(a_t\\)} that generates a state sequence {\\(s_t\\)} start from \\(s_o\\) minimizes    

$$
\min h (s_r)+\sum _{t=0}^{T-1} h(s_t,a_t)
$$

> &#x2705; å› ä¸ºæŠŠæ—¶é—´ç¦»æ•£åŒ–ï¼Œæ­¤å¤„ç”¨æ±‚å’Œä¸ç”¨ç§¯åˆ†ã€‚    

subject to   

$$
\begin{matrix}
 f(s_t,a_t)-s_{t+1}=0\\\\
\text{ for } 0 \le t < T
\end{matrix}
$$

> &#x2705; è¿åŠ¨å­¦æ–¹ç¨‹ï¼Œä½œä¸ºçº¦æŸ    

### è½¬åŒ–ä¸ºä¼˜åŒ–é—®é¢˜

The Lagrange function   

$$
L(s,a,\lambda ) = h(s _ T)+ \sum _ {t=0} ^ {T-1} h(s _t,a _t) + \lambda _ {t+1}^T(f(s _t,a _t) - s _ {t+1})
$$


P27   
### æ±‚è§£æ‹‰æ ¼æœ—æ—¥æ–¹ç¨‹


![](./assets/12-10-1.png)  


> &#x2705; æ‹‰æ ¼æœ—æ—¥æ–¹ç¨‹ï¼Œå¯¹æ¯ä¸ªå˜é‡æ±‚å¯¼ï¼Œå¹¶ä»¤å¯¼æ•°ä¸ºé›¶ã€‚å› æ­¤å¾—åˆ°å³è¾¹æ–¹ç¨‹ç»„ã€‚  
> &#x2705; å³è¾¹æ–¹ç¨‹ç»„è¿›ä¸€æ­¥æ•´ç†ï¼Œå¾—åˆ°å·¦è¾¹ã€‚  
> &#x2705; \\(\lambda \\) ç±»ä¼¼äºé€†å‘ä»¿çœŸã€‚   
> &#x2705; å…¬å¼ 3ï¼šé€šè¿‡è½¬ä¸ºä¼˜åŒ–é—®é¢˜æ±‚ \\(a\\)ï¼   

P30  
### Pontryaginâ€™s Maximum Principle for discrete systems


![](./assets/12-11.png)  

![](./assets/12-12.png)  


> &#x2705; æ–¹ç¨‹ç»„æ•´ç†å¾—åˆ°å·¦è¾¹ï¼Œç§°ä¸º PMP æ¡ä»¶ã€‚æ˜¯å¼€ç¯æ§åˆ¶æœ€ä¼˜çš„å¿…è¦æ¡ä»¶ã€‚    



P32   
## Optimal Control

**Open-loop Control**:    
given a start state \\(s_0\\), compute sequence of actions {\\(a_t\\)} to reach the goal   


![](./assets/12-13.png)  

>  **Shooting method** directly applies PMP. However, it does not scale well to complicated problems such as motion controlâ€¦   
\\(<br>\\)   
Need to be combined with collocation method, multiple shooting, etc. for those problems.    
\\(<br>\\)   
Or use derivative-free approaches.   

![](./assets/12-14.png)  


> &#x2705; å¯¹äºå¤æ‚å‡½æ•°ï¼Œè¡¨ç°æ¯”è¾ƒå·®ï¼Œè¿˜éœ€è¦å€ŸåŠ©å…¶å®ƒæ–¹æ³•ã€‚    

# é—­ç¯æ§åˆ¶

![](./assets/12-05.png)  

P34  
## Dynamic Programming   

![](./assets/12-15.png) 

å¸Œæœ›æ‰¾åˆ°ä¸€æ¡æœ€çŸ­è·¯å¾„åˆ°è¾¾å¦ä¸€ä¸ªç‚¹ï¼Œå¯¹è¿™ä¸ªé—®é¢˜ç”¨ä¸åŒçš„æ–¹å¼å»ºæ¨¡ï¼Œä¼šå¾—åˆ°ä¸åŒçš„æ–¹æ³•ï¼š  

||||
|---|---|---|
|åŠ¨æ€è§„åˆ’é—®é¢˜|Find a path {\\(s_t\\)} that minimizes |\\(J(s_0)=\sum _ {t=0}^{ } h(s_t,s_{t+1})\\)|
|è½¨è¿¹é—®é¢˜|Find a sequence of action {\\(a_t\\)} that minimizes |  \\(J(s_0)=\sum _ {t=0}^{ } h(s_t,a_t)\\)<br> subject to <br> \\( s_{t+1}=f(s_t,a_t)\\)|
|æ§åˆ¶ç­–ç•¥é—®é¢˜|Find a policy \\( a_t=\pi (s_t,t)\\)æˆ– \\( a_t=\pi (s_t)\\)that minimizes|\\(J(s_0)=\sum _ {t=0}^{ } h(s_t,a_t)\\)<br>subject to  <br>\\(s_{t+1}=f(s_t,a_t)\\)

P39   
## Bellmanâ€™s Principle of Optimality   

> &#x2705; é’ˆå¯¹æ§åˆ¶ç­–ç•¥é—®é¢˜ï¼Œä»€ä¹ˆæ ·çš„ç­–ç•¥æ˜¯æœ€ä¼˜ç­–ç•¥ï¼Ÿ  

![](./assets/12-16.png) 

An optimal policy has the property that whatever the initial 
state and initial decision are, the remaining decisions must 
constitute an optimal policy with regard to the state resulting 
from the first decision.   

\\(^\ast \\) The problem is said to have **optimal substructure**    


P40   
## Value Function  

Value of a state \\(V(s)\\) :    

 - the minimal total cost for finishing the task starting from \\(s\\)   
 - the total cost for finishing the task starting from \\(s\\) using the optimal policy    



> &#x2705; Value Funcronï¼Œè®¡ç®—ä»æŸä¸ªç»“ç‚¹åˆ° gool çš„æœ€å°ä»£ä»·ã€‚   
> &#x2705; åé¢åŠ¨æ€è§„åˆ’åŸç†è·³è¿‡ã€‚   



P49   
## The Bellman Equation   

Mathematically, an optimal **value function** \\(V(s)\\) can be defined recursively as:   

$$
V(s)=\min_{a} (h(s,a)+V(f(s,a)))
$$

> &#x2705; hä»£è¡¨sçŠ¶æ€ä¸‹æ‰§è¡Œä¸€æ­¥açš„ä»£ä»·ã€‚fä»£è¡¨ä»¥sçŠ¶æ€ä¸‹æ‰§è¡Œä¸€æ­¥aä¹‹åçš„çŠ¶æ€ã€‚  

If we know this value function, the optimal **policy** can be computed as   

$$
\pi (s)=\arg \min_{a} (h(s,a)+V(f(s,a)))
$$

> &#x2705; piä»£è¡¨ä¸€ç§ç­–ç•¥ï¼Œæ ¹æ®å½“å‰çŠ¶æ€sæ‰¾åˆ°æœ€ä¼˜çš„ä¸‹ä¸€æ­¥aã€‚  
> &#x2705; This arg max can be easily computed for discrete control problems.   
But there are not always closed-forms solution for continuous control problems.   

or   

$$
\begin{matrix}
 \pi (s)=\arg \min_{a} Q(s,a)\\\\
\text{where} \quad \quad  Q(s,a)=h(s,a)+V(f(s,a))
\end{matrix}
$$


Q-functionç§°ä¸ºState-action value function    
Learning \\(V(s)\\) and/or \\(Q(s,a)\\) is the core of optimal control / reinforcement learning methods   
> &#x2705; å¼ºåŒ–å­¦ä¹ æœ€ä¸»è¦çš„ç›®çš„æ˜¯å­¦ä¹  \\(V\\) å‡½æ•°å’Œ \\(Q\\) å‡½æ•°ï¼Œå¦‚æœ \\(a\\) æ˜¯æœ‰é™çŠ¶æ€ï¼Œéå†å³å¯ã€‚ä½†åœ¨è§’è‰²åŠ¨ç”»é‡Œï¼Œ\\(a\\) æ˜¯è¿ç»­çŠ¶æ€ã€‚  



P52   
# Linear Quadratic Regulator (LQR)   

![](./assets/12-17.png) 


 - LQR is a special class of optimal control problems with   
    - **Linear** dynamic function   
    - **Quadratic** objective function   


> &#x2705; LQR æ˜¯æ§åˆ¶é¢†åŸŸä¸€ç±»ç»å…¸é—®é¢˜ï¼Œå®ƒå¯¹åŸæ§åˆ¶é—®é¢˜åšäº†ä¸€äº›ç‰¹å®šçš„çº¦æŸã€‚å› ä¸ºç®€åŒ–äº†é—®é¢˜ï¼Œå¯ä»¥å¾—åˆ°æœ‰ç‰¹å®šå…¬å¼çš„ \\(Q\\) å’Œ \\(V\\).    


P53   
## A very simple example   

### é—®é¢˜æè¿°

![](./assets/12-18.png) 

Compute a target trajectory \\(\tilde{x}(t)\\) such that the simulated trajectory \\(x(t)\\) is a sine curve.   


![](./assets/12-19.png) 

$$
\min _{(x_n,v_n,\tilde{x} _n)} \sum _{n=0}^{N} (\sin (t_n)-x_n)^2+\sum _{n=0}^{N}\tilde{x}^2_n 
$$

$$
\begin{align*}
 s.t. \quad \quad v _ {n+1} & = v _ n + h(k _p ( \tilde{x} _ n - x _ n) - k _ dv _ n ) \\\\
 v _ {x+1} & = x _ n + hv _ {n+1}
\end{align*}
$$


P54  
objective function   

$$
\min s^T_TQ_Ts_T+\sum_{t=0}^{T} s^T_tQ_ts_t+a^T_tR_ta_t
$$

subject to dynamic function     

$$
s_{t+1}=A_ts_t+B_ta_t   \quad \quad \text{for }   0\le t <T 
$$

> &#x2705; ç›®æ ‡å‡½æ•°æ˜¯äºŒæ¬¡å‡½æ•°ï¼Œè¿åŠ¨å­¦æ–¹ç¨‹æ˜¯çº¿æ€§å‡½æ•°ã€‚è¿™æ˜¯ä¸€ä¸ªå…¸å‹çš„ LQR é—®é¢˜ã€‚  

P58   
### æ¨å¯¼ä¸€æ­¥

> &#x2705; ç”±äºå­˜åœ¨optimal substructureï¼Œæ¯æ¬¡åªéœ€è¦è€ƒè™‘ä¸‹ä¸€ä¸ªçŠ¶æ€çš„æœ€ä¼˜è§£ã€‚  
> &#x2705; æ¯ä¸€ä¸ªçŠ¶æ€åŸºäºä¸‹ä¸€ä¸ªçŠ¶æ€æ¥è®¡ç®—ï¼Œä¸æ–­å¾€ä¸‹è¿­ä»£ï¼Œç›´åˆ°æœ€åä¸€ä¸ªçŠ¶æ€ã€‚  
> &#x2705; æœ€åä¸€ä¸ªçŠ¶æ€çš„Vçš„è®¡ç®—ä¸aæ— å…³ã€‚  
> &#x2705; è®¡ç®—å®Œæœ€åä¸€ä¸ªï¼Œå†è®¡ç®—å€’æ•°ç¬¬äºŒä¸ªï¼Œä¾æ¬¡å¾€å‰æ¨ã€‚  

![](./assets/12-20-1.png) 


P60   
å…¬å¼æ•´ç†å¾—ï¼š  

![](./assets/12-20.png) 


P61 
![](./assets/12-21.png) 

> &#x2705; ç»“è®ºï¼šæœ€ä¼˜ç­–ç•¥ä¸å½“å‰çŠ¶æ€çš„å…³ç³»æ˜¯çŸ©é˜µKçš„å…³ç³»ã€‚  

P62   
å½“aå–æœ€å°å€¼æ—¶ï¼Œæ±‚å‡ºVï¼š

![](./assets/12-22.png)  

> &#x2705; \\(V(S_{T-1})\\)å’Œ\\(V(S_{T})\\)çš„å½¢å¼åŸºæœ¬ä¸€è‡´ï¼Œåªæ˜¯Pçš„è¡¨ç¤ºä¸åŒã€‚ 

P63  
### æ¨å¯¼æ¯ä¸€æ­¥


![](./assets/12-23.png) 

P64   
### Solution

 - LQR is a special class of optimal control problems with   
    - Linear dynamic function   
    - Quadratic objective function   
 - Solution of LQR is a linear feedback policy  

![](./assets/12-24.png) 


P65   
## æ›´å¤æ‚çš„æƒ…å†µ

 - How to deal with   
    - Nonlinear dynamic function?   
    - Non-quadratic objective function?   


> &#x2705; äººä½“è¿åŠ¨æ¶‰åŠåˆ°è§’åº¦æ—‹è½¬ï¼Œå› æ­¤æ˜¯éçº¿æ€§çš„ã€‚  



P68  
### Nonlinear problems   

![](./assets/12-25.png) 

> &#x2705; æ–¹æ³•ï¼šæŠŠé—®é¢˜è¿‘ä¼¼ä¸ºçº¿æ€§é—®é¢˜ã€‚  

Approximate cost function as a quadratic function:   

> &#x2705; ç›®æ ‡å‡½æ•°ï¼šæ³°å‹’å±•å¼€ï¼Œä¿ç•™äºŒæ¬¡ã€‚  

$$
h(s_t,a_t)\approx h(\bar{s}_t ,\bar{a}_t)+\nabla h(\bar{s}_t ,\bar{a}_t)\begin{bmatrix}
 s_t-\bar{s} _t\\\\
a_t-\bar{a} _t
\end{bmatrix} + \frac{1}{2} \begin{bmatrix}
 s_t-\bar{s} _t\\\\
a_t-\bar{a} _t
\end{bmatrix}^T\nabla^2h(\bar{s}_t ,\bar{a}_t)\begin{bmatrix}
 s_t-\bar{s} _t\\\\
a_t-\bar{a} _t
\end{bmatrix}
$$

Approximate dynamic function as a linear function:    

> &#x2705; è½¬ç§»å‡½æ•°ï¼šæ³°å‹’å±•å¼€ï¼Œä¿ç•™ä¸€æ¬¡æˆ–äºŒæ¬¡ã€‚  

$$
f(s_t,a_t)\approx f(\bar{s}_t ,\bar{a}_t)+\nabla f(\bar{s}_t ,\bar{a}_t)\begin{bmatrix}
 s_t-\bar{s} _t\\\\
a_t-\bar{a} _t
\end{bmatrix}
$$

å±•å¼€ä¸ºä¸€æ¬¡é¡¹ï¼Œå¯¹åº”è§£å†³ç®—æ³•ï¼šiLQRï¼ˆiterative LQRï¼‰ 


Or a quadratic function:   

$$
f(s_t,a_t)\approx \ast \ast \ast \frac{1}{2} \begin{bmatrix}
 s_t-\bar{s} _t\\\\
a_t-\bar{a} _t
\end{bmatrix}^T\nabla^2f(\bar{s}_t ,\bar{a}_t)\begin{bmatrix}
 s_t-\bar{s} _t\\\\
a_t-\bar{a} _t
\end{bmatrix}
$$

å±•å¼€ä¸ºäºŒæ¬¡é¡¹ï¼Œå¯¹åº”è§£å†³ç®—æ³•ï¼šDDPï¼ˆDifferential Dynamic Programmingï¼‰

P69  
### ç›¸å…³åº”ç”¨

> &#x1F50E; [Muico et al 2011 - Composite Control of Physically Simulated Characters]   

> &#x2705; é€‰æ‹©åˆé€‚çš„ \\(Q\\) å’Œ \\(R\\)ï¼Œéœ€è¦ä¸€äº›å·¥ç¨‹ä¸Šçš„æŠ€å·§ã€‚   
> &#x2705; ä¸ºäº†æ±‚è§£æ–¹ç¨‹ï¼Œéœ€è¦æ˜¾å¼åœ°å»ºæ¨¡è¿åŠ¨å­¦æ–¹ç¨‹ã€‚  



P70  
## Model-based Method vs. Model-free Method   

> &#x2705; Model Based æ–¹æ³•ï¼Œè¦æ±‚ dynamic function æ˜¯å·²çŸ¥çš„ï¼Œä½†æ˜¯å®é™…ä¸Šè¿™ä¸ªå‡½æ•°å¯èƒ½æ˜¯ï¼ˆ1ï¼‰æœªçŸ¥çš„ï¼ˆ2ï¼‰ä¸ç²¾ç¡®çš„ã€‚    
> &#x2705; å› æ­¤Model Based æ–¹æ³•å¯¹äºå¤æ‚é—®é¢˜éš¾ä»¥åº”ç”¨ï¼Œä½†å¯¹äºç®€å•é—®é¢˜éå¸¸é«˜æ•ˆã€‚  

What if the dynamic function \\(f(s,a)\\) is not know?  

> &#x2705; \\(f\\) æœªçŸ¥åªæ˜¯æŠŠ \\(f\\) å½“æˆä¸€ä¸ªé»‘ç›’å­ï¼Œä»éœ€è¦æ ¹æ® \\(S_t\\) å¾—åˆ° \\(S_{tï¼‹1}\\) .   

What if the dynamic function \\(f(s,a)\\) is not accurate?    

> &#x2705; ä¸å‡†ç¡®æ¥æºäºï¼ˆ1ï¼‰æµ‹è¯•é‡è¯¯å·®ï¼ˆ2ï¼‰é—®é¢˜ç®€åŒ–

What if the system has noise?    

What if the system is highly nonlinear?     

P72  
# Sampling-based Policy Optimization   

 - Iterative methods
    - Goal: find the optimal **policy** \\(\pi (s;\theta )\\) that minimize the objective \\(J(\theta )=\sum_{t=0}^{}h(s_t,a_t) \\)     
    - Initialize policy parmeters \\(\pi (x;\theta )\\)   
    - Repeat:   
      - Propose a set of candidate parameters {\\(\theta _i \\)} according to \\(\theta \\)    
      - Simulate the agent under the control of each \\( \pi ( \theta _i)\\) 
      - Evaluate the objective function \\( J (\theta_i )\\)  on the simulated state-action sequences    
      - Update the estimation of \\(\theta \\) based on {\\( J (\theta_i )\\)}     

 - Example: CMA-ES


> &#x2705; åŸºäºé‡‡æ ·çš„æ–¹æ³•ã€‚  

P73   
## Example: Locomotion Controller with Linear Policy

> &#x1F50E; [Liu et al. 2012 â€“ Terrain Runner]

P74  
### Stage 1a: Open-loop Policy   

Find open-loop control using SAMCON

![](./assets/12-26.png) 


> &#x2705; ä½¿ç”¨å¼€ç¯è½¨è¿¹ä¼˜åŒ–å¾—åˆ°å¼€ç¯æ§åˆ¶è½¨è¿¹ã€‚    



P76  
### Stage 1b: Linear Feedback Policy

![](./assets/12-27.png)   


> &#x2705; ä½¿ç”¨åé¦ˆæ§åˆ¶æ›´æ–°æ§åˆ¶ä¿¡å·ã€‚ç”±äºå‡è®¾äº†çº¿æ€§å…³ç³»ï¼Œæ ¹æ®åç¦» offset å¯ç›´æ¥å¾—åˆ°è°ƒæ•´ offset.  




P78   
### Stage 1b: Reduced-order Closed-loop Policy

![](./assets/12-28.png)  

> &#x2705; æŠŠ \\(M\\) åˆ†è§£ä¸ºä¸¤ä¸ªçŸ©é˜µï¼Œ\\(M_{AXB} = M_{AXC}\cdot M_{CXB}\\) å¦‚æœ \\(C\\) æ¯”è¾ƒå°ï¼Œå¯ä»¥æ˜æ˜¾å‡å°‘çŸ©é˜µçš„å‚æ•°é‡ã€‚    
> &#x2705; å¥½å¤„ï¼š(1) å‡å°‘å‚æ•°ï¼Œå‡åŒ–ä¼˜åŒ–è¿‡ç¨‹ã€‚(2) æŠ¹æ‰çŠ¶æ€é‡Œä¸éœ€è¦çš„ä¿¡æ¯ã€‚  



P79   
#### Manually-selected States: s   

 - Running: 12 dimensions


![](./assets/12-29.png) 


> &#x2705; ï¼ˆ1ï¼‰æ ¹ç»“ç‚¹æ—‹è½¬ï¼ˆ2ï¼‰è´¨å¿ƒä½ç½®ï¼ˆ3ï¼‰è´¨å¿ƒé€Ÿåº¦ï¼ˆ4ï¼‰æ”¯æ’‘è„šä½ç½®   



P80  
#### Manually-selected Controls: a  

 - for all skills: 9 dimensions   


![](./assets/12-30.png)   


> &#x2705; ä»…å¯¹å°‘æ•°å…³èŠ‚åŠ åé¦ˆã€‚   



P81   
### Optimization

$$
\delta a=M\delta s+\hat{a} 
$$

 - Optimize \\(M\\)   
    - CMA, Covariance Matrix Adaption ([Hansen 2006])
    - For the running task:
      - #optimization variables: \\(12 ^\ast 9 = 108 / (12^\ast 3+3 ^\ast 9) = 63\\)   
     - 12 minutes on 24 cores   


P85  
# Optimal Control \\(\Leftrightarrow \\) Reinforcement Learning   


â€¢ RL shares roughly the same overall goal with Optimal Control   

$$
\max \sum_{t=0}^{} r (s_t,a_t)
$$

> &#x2705; ç›¸åŒç‚¹ï¼šç›®æ ‡å‡½æ•°ç›¸åŒï¼Œæ˜¯æ¯ä¸€æ—¶åˆ»çš„ä»£ä»·å‡½æ•°ä¹‹å’Œã€‚ 

â€¢ But RL typically does not assume perfect knowledge of system   

 ![](./assets/12-30-1.png) 

> &#x2705; æœ€ä¼˜æ§åˆ¶è¦æ±‚æœ‰ç²¾ç¡®çš„è¿åŠ¨æ–¹ç¨‹ï¼Œè€Œ RL ä¸éœ€è¦ã€‚  

 - RL can still take advantage of a system model â†’ model-based RL   
    - The model can be learned from data   
$$
s_{t+1}=f(s_t,a_t;\theta )
$$

> &#x2705; RL é€šè¿‡ä¸æ–­ä¸ä¸–ç•Œäº¤äº’è¿›è¡Œé‡‡æ ·ã€‚   
  

P87   
## Markov Decision Process (MDP)  


![](./assets/12-32.png)   


![](./assets/12-31.png)   

|||
|---|---|
|State|  \\(\quad s_t \quad \quad \\)|
|Action| \\(\quad a_t\\)|
|Policy |\\(\quad \quad a_t\sim \pi (\cdot \mid s_t)\\)   |
|Transition probability  |\\(\quad \quad s_{t+1}\sim p  (\cdot \mid s_t,a_t)\\)|   
|Reward  |\\(\quad \quad r_t=r (s_t,a_t)\\)|   
|Return| \\(R = \sum _{t}^{} \gamma ^t r (s_t,a_t)\\)|   



> &#x2705; çœŸå®åœºæ™¯ä¸­è½¨è¿¹æ— é™é•¿ï¼Œä¼šå¯¼åˆ° \\(R\\) æ— é™å¤§ã€‚   
> &#x2705; å› æ­¤ä¼šä½¿ç”¨å°äº 1 çš„ \\(r,t\\) è¶Šå¤§åˆ™å¯¹ç»“æœçš„å½±å“è¶Šå°ã€‚   



P88  
## è·Ÿè¸ªé—®é¢˜å˜æˆMDPé—®é¢˜   

Trajectory    

$$
\begin{matrix}
  \tau =& s_0 & a_0 & s_1 & a_1 & s_2&\dots 
\end{matrix}
$$

Reward   

![](./assets/12-33.png)   



P90   
## MDPé—®é¢˜çš„æ•°å­¦æè¿°

> &#x2705; Markov æ€§è´¨ï¼šå½“å½“å‰çŠ¶æ€å·²çŸ¥çš„æƒ…å†µä¸‹ï¼Œä¸‹ä¸€æ—¶åˆ»çŠ¶æ€åªä¸å½“å‰çŠ¶æ€ç›¸å…³ï¼Œè€Œä¸ä¸ä¹‹å‰ä»»ä¸€æ—¶åˆ»çŠ¶æ€ç›¸å…³ã€‚   

MDP is a **discrete-time** stochastic control process.    
It provides a mathematical framework for modeling decision making in situations     
where outcomes are **partly random and partly under the control of a decision maker**.   

A MDP problem:    

\\(\mathcal{M}\\) = {\\(S,A,p,r\\)}    
\\(S\\): state space   
\\(A\\): action space   
pï¼šçŠ¶æ€è½¬ç§»æ¦‚ç‡ï¼Œå³è¿åŠ¨å­¦æ–¹ç¨‹ã€‚   
rï¼šä»£ä»·å‡½æ•°ã€‚   



P91  

Solve for a policy \\(\pi (a\mid s)\\) that optimize the **expected return**    


$$
J=E[R]=E_{\tau \sim \pi }[\sum_{t}^{} \gamma ^tr(s_t,a_t)]
$$

> &#x2705; æ±‚è§£ä¸€ä¸ªpolicy \\(\pi \\) ä½¿æœŸæœ›æœ€ä¼˜ï¼Œè€Œä¸æ˜¯ç›´æ¥æ‰¾æœ€ä¼˜è§£ã€‚   

Overall all trajectories \\(\tau \\) = { \\(s_0, a_0 , s_1 , a_1 ,  \dots  \\)} induced by \\(\pi \\)   

> &#x2705; å‡è®¾ \\(\pi \\) å‡½æ•°å’Œ \\(p\\) å‡½æ•°éƒ½æ˜¯æœ‰å™ªéŸ³çš„ï¼Œå³å¾—åˆ°çš„ç»“æœä¸æ˜¯ç¡®å®šå€¼ï¼Œè€Œæ˜¯ä»¥ä¸€å®šæ¦‚ç‡å¾—åˆ°æŸä¸ªç»“æœã€‚**è¿™æ˜¯ä¸æœ€ä¼˜æ§åˆ¶é—®é¢˜çš„åŒºåˆ«ã€‚**   

P93   
## Bellman Equations

In optimal control:    

![](./assets/12-34.png)   

In RL control:    

![](./assets/12-35.png)   

> &#x2705; æ­¤å¤„çš„\\(\pi \\)æ˜¯æŸä¸€ä¸ªç­–ç•¥ï¼Œè€Œä¸æ˜¯æœ€ä¼˜ç­–ç•¥ã€‚  

P94   
## How to Solve MDP  

### Value-based Methods

- Learning the value function/Q-function using the Bellman equations   
- Evaluation the policy as    

$$
\pi (s) = \arg \min_{a} Q(s,a)
$$

- Typically used for **discrete** problems   
- Example: Value iteration, Q-l a ning, DQN, â€¦   

P95   
> &#x1F50E; DQN [Mnih et al. 2015, Human-level control through deep reinforcement learning]   

P96  

### ç›¸å…³å·¥ä½œ

> &#x1F50E; [Liu et al. 2017: Learning to Schedule Control Fragments ]   

![](./assets/12-36.png)   

> &#x2705; DQN æ–¹æ³•è¦æ±‚æ§åˆ¶ç©ºé—´å¿…é¡»æ˜¯ç¦»æ•£çš„ï¼Œä½†çŠ¶æ€ç©ºé—´å¯ä»¥æ˜¯è¿ç»­çš„ã€‚  
> &#x2705; å› æ­¤å¯ç”¨äºé«˜é˜¶çš„æ§åˆ¶ã€‚  

P97   
### Policy Gradient approach
- Learning the value function/Q-function using the Bellman equations   
- Compute approximate **policy gradient** according to value functions using Monte-Carlo method   

- Update the policy using policy gradient  

- Suitable for **continuous** problems   

- Exa pl : REINFORCE, TRPO, PPO, â€¦   

> &#x2705; policy grodient æ˜¯ Value function å¯¹çŠ¶æ€å‚æ•°çš„æ±‚å¯¼ã€‚ä½†è¿™ä¸ªæ²¡æ³•ç®—ï¼Œæ‰€ä»¥ç”¨ç»Ÿè®¡çš„æ–¹æ³•å¾—åˆ°è¿‘ä¼¼ã€‚   
> &#x2705; ç‰¹ç‚¹æ˜¯æ˜¾ç¤ºå®šä¹‰ Dolicy å‡½æ•°ã€‚å¯¹è¿ç»­é—®é¢˜æ›´æœ‰æ•ˆã€‚    


P98   
### ç›¸å…³å·¥ä½œ

||||
|--|--|--|
| ![](./assets/12-37.png)   |  ![](./assets/12-38.png)   |  ![](./assets/12-39.png)   | 
| [Liu et al. 2016. ControlGraphs] | [Liu et al. 2018]   | [Peng et al. 2018. DeepMimic] |  



P100  
## Generative Control Policies   

> &#x2705; ä½¿ç”¨RL learningï¼ŒåŠ ä¸Šä¸€ç‚¹ç‚¹è½¨è¿¹ä¼˜åŒ–çš„æ§åˆ¶ï¼Œå°±å¯ä»¥å®ç°éå¸¸å¤æ‚çš„åŠ¨ä½œã€‚  

> &#x1F50E; [Yao et al. Control VAE]   



P101  
# Whatâ€™s Next?   

## Digital Cerebellum

Large Pretrained Model for Motion Control

![](./assets/12-40.png)   


P102  
## Cross-modality Generation

- \\(\Leftrightarrow\\) LLM \\(\Leftrightarrow\\) Text/Audio \\(\Leftrightarrow\\) Motion/Control \\(\Leftrightarrow\\) Image/Video \\(\Leftrightarrow\\)     
- Digital Actor?    


---------------------------------------
> æœ¬æ–‡å‡ºè‡ªCaterpillarStudyGroupï¼Œè½¬è½½è¯·æ³¨æ˜å‡ºå¤„ã€‚
>
> https://caterpillarstudygroup.github.io/GAMES105_mdbook/